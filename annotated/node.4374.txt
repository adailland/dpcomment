>>>mcollina, Member


This PR adds support for:




socket.send(buf, port, host, [, callback])


socket.send([buf1, buf2, ... ], port, host, [, callback])




Fixes #4302.


This is probably missing some benchmarks, and might like an optimization pass, but I would like to get some feedbacks first :).


cc @ronkorving @indutny @saghul


>>>indutny, Owner


I guess arguments fit on the same line? ;)


>>>indutny, Owner


[ buffer ]


>>>indutny, Owner



;##D1 What should be done about the string encoding?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

What should we do about encoding?


>>>indutny, Owner



;##D2 Did we support string?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL ELAB D1
;##SEN NEU

Did we actually support strings here?


>>>indutny, Owner



;##D3 It seems to be inconsistent with core
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D2
;##SEN NEU

I think we didn't, I would not introduce APIs that convert strings to Buffers without using encoding. This seems to be inconsistent with the rest of the core.


>>>mcollina, Member

;##D4 Strings without encoding are bad, but not new
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL REFR D3
;##SEN NEU

dgram supports strings without encoding at this point. I'm up for supporting only Buffers in this new API.
Strings without encoding are just "bad".


Original: https://github.com/nodejs/node/blob/master/lib/dgram.js#L253-L254


>>>indutny, Owner


Ah, I see it now. Yeah, this is weird, but I agree that we may fix it later.


>>>mcollina, Member


Given that this API signature is new, I would fix it now if possible.

;##D5 Is there any other option other than supporting only buffer parameters?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL ELAB D4
;##SEN NEU

I'm ðŸ‘ to support only buffers in the 4 parameters send (the new one).
Any other opinions?


>>>indutny, Owner


Please put it on the same line, it is going to fit here now.


>>>indutny, Owner



;##D6 This might affect performances
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU
;##D7 Can we pass it to the C++ layer?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL ELAB D6
;##SEN NEU

I feel like it could be potentially slower and will do excessive allocation. Perhaps we should just pass offset to the C++ layer?


>>>mcollina, Member

;##D8 There are problems in supporting both APIs through the same C++ function
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D7
;##SEN NEU

;##D9 Here are two solutions on how we could solve it
;##ROLE OP
;##INV F
;##BCOM T
;##FORM ENU
;##REL ELAB D8
;##SEN NEU

I agree, that was done before and I cut it from here to get some feedback. However, I am struggling a bit to support both APIs through the same C++ function.
Should I (partially) duplicate that, or should I use positional argument in the array for offset and length for each buffer?
Or should I allocate a new object for each of this? You probably have some idea of what will behave better.


Basically I can pass [buf1, offset1, length1, buf2, offset2, length2, ...].


However I am not entirely sure this is worth the effort, but me and @mscdex disagree on this: #4302 (comment) #4302 (comment)


>>>ronkorving, Contributor



;##D10 Offset and length become useless if you send an array of buffers
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D9
;##SEN NEU

If you're gonna send an array of buffers, I have no idea what the point of offset and length should be. It seems a rather useless feature (hence my initial proposal for a method called sendv instead). I honestly have no opinion as to what the semantics here should be, but if you can find any that make any sense at all, go for it. I don't think unix's sendmsg function actually takes these arguments when sending msg_iov.



;##D11 Sending a single buffer should not incurr in a slice penalty
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D9
;##SEN NEU

Now as far the "old" case when sending a single buffer, I don't think it should be incurring a slice penalty. Especially since in many calls offset will be 0, and length will be the total length anyway (those cases should be optimized out). The fact is though, that libuv only really seems to support sendmsg with a message object, and not a call with offset and length. The fact that Node exposes these may be legacy reasons (I don't know the history of it but can imagine that before libuv the sendmsg syscall was used differently).



;##D12 Sending a single buffer should not have a greater cost than sendinng an array
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D11
;##SEN NEU

So long story short: using offset and length with an array of buffers is a weird pattern, but some semantics need to be found (eg: slicing while considering offset to apply to the first buffer(s), and length to apply to the buffers' collective size) and building a new array could do the trick). When sending a non-array however, cost should be no greater than it used to be.


>>>mcollina, Member




If you're gonna send an array of buffers, I have no idea what the point of offset and length should be. It seems a rather useless feature (hence my initial proposal for a method called sendv instead). I honestly have no opinion as to what the semantics here should be, but if you can find any that make any sense at all, go for it. I don't think unix's sendmsg function actually takes these arguments when sending msg_iov.





;##D13 I'd avoid duplicating code in either cases
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D10
;##SEN NEu

I would really try to avoid code duplication for both cases, as I would not like to maintain both a 'send' and 'sendv' in C++ land. This is basically why I am asking this question. This is only relevant to the internal C++ API, not to the user-facing API, which would just be an array of buffers.




The fact is though, that libuv only really seems to support sendmsg with a message object, and not a call with offset and length. The fact that Node exposes these may be legacy reasons (I don't know the history of it but can imagine that before libuv the sendmsg syscall was used differently).





;##D14 There are reasons to use offset and lenght
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL REFR D10
;##SEN NEW

This is not correct. The point where we are currently using offset and length in C++ is: https://github.com/nodejs/node/blob/master/src/udp_wrap.cc#L267-L268. That structure that goes deep down into sendmsg, and it is where the magic happens. This is faster than slicing in JS land, however I have no idea how faster it is.



;##D15 UDP makes ofset and lenght useless
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL REFR D14
;##SEN NEU

IMHO, having offset and length are kind of useless anyway because of UDP. Does anybody has a lib that is using those heavily to test it?


>>>indutny, Owner



;##D16 We should use slice here
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D11
;##SEN NEU

We have similar APIs in fs module as well. I think I agree with using slice here, the whole API seems to be not that necessary, and I doubt that it is used that much. Thanks!


>>>ronkorving, Contributor


Great start, thanks a ton :) What do the benchmarks say at this time?


>>>mcollina, Member


Current benchmarks comes out slightly slower on the "old" API. These are possibly due to: #4374 (diff)


before:
net/dgram.js len=1 num=100 type=send dur=5: 0.00120
net/dgram.js len=1 num=100 type=recv dur=5: 0.00020
net/dgram.js len=64 num=100 type=send dur=5: 0.07730
net/dgram.js len=64 num=100 type=recv dur=5: 0.01255
net/dgram.js len=256 num=100 type=send dur=5: 0.30837
net/dgram.js len=256 num=100 type=recv dur=5: 0.04958
net/dgram.js len=1024 num=100 type=send dur=5: 0.71537
net/dgram.js len=1024 num=100 type=recv dur=5: 0.12022


After:
net/dgram.js len=1 num=100 type=send dur=5: 0.00106
net/dgram.js len=1 num=100 type=recv dur=5: 0.00018
net/dgram.js len=64 num=100 type=send dur=5: 0.06785
net/dgram.js len=64 num=100 type=recv dur=5: 0.01109
net/dgram.js len=256 num=100 type=send dur=5: 0.27151
net/dgram.js len=256 num=100 type=recv dur=5: 0.04330
net/dgram.js len=1024 num=100 type=send dur=5: 0.70451
net/dgram.js len=1024 num=100 type=recv dur=5: 0.11787


I have no benchmarks yet on the new APIs.


>>>thefourtheye, Contributor



;##D17 Should use common.mustCall for safety here
;##ROLE ETC
;##INV F
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Use common.mustCall here, so that it will make sure that the test will fail if the callback is not invoked.


>>>thefourtheye, Contributor



;##D18 This apporach migth not work with slow tests
;##ROLE ETC
;##INV F
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Why do we need this? We have few test boxes which run very slowly compared to the others. So, this 200 might not work well with all of them.


>>>mcollina, Member

;##D19 Its purpose is to assure the callback is called whithin a define timeframe
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL REFR D18
;##SEN NEU

I copied it over from the other dgram tests (the same applies to common.mustCall).
The whole purpose it making sure that the callback is called within a definite timeframe. We can move this to 1 or 2 seconds without issues.


Should I submit a different PR for some test cleanup for all of dgram tests?


>>>thefourtheye, Contributor



;##D20 Should be using common.platformTimeout instead then
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D19
;##SEN NEU

@mcollina I think you should use common.platformTimeout here.


>>>mcollina, Member



;##D21 Should the other tests be amended too?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

Great @thefourtheye, should I amend the other tests as well?


>>>thefourtheye, Contributor



;##D22 Don't know why those tests are timebound
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

I am actually not sure why these tests are time-bound.


>>>thefourtheye, Contributor



;##D23 common.localhostIPv4 should be used instead of the IP address
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Don't use the IP address as it is. Use common.localhostIPv4


>>>mcollina, Member



;##D24 Should the other tests be amended too?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

should I amend the other tests as well? All the dgram tests follows the same convention.


>>>thefourtheye, Contributor



;##D25 Hardcoded IPs should be avoided as much as possible
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D24
;##SEN NEU

Yes. We should avoid using the hardcoded IPs unless absolutely necessary.


>>>thefourtheye, Contributor


Unused.


>>>thefourtheye, Contributor


Unused.


>>>indutny, Owner


const?


>>>mcollina, Member


@thefourtheye @indutny I have updated the tests to follow the current conventions. I will update also the other test to the same on another PR.


>>>saghul, Member



;##D26 It would be nice to use uv_udp_try_send
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Not on this PR, but it would probably be nice to use uv_udp_try_send here, just in case we can send it on the spot.


>>>mcollina, Member

;##D27 Any issue if we switch to uv_udp_try_send here?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL ELAB D26
;##SEN NEU

;##D28 What changes with uv_udp_try_send?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL ELAB D26
;##SEN NEU

I would do that after this land. Do you see any issue with switching to uv_udp_try_send?
What is the difference anyway? on what conditions it can be sent on the spot?


>>>saghul, Member



;##D29 It wouldn't be a switch, just ot send it immediately only if the socket is writable
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D28
;##SEN NEU

It's not switching, the idea is to use it to try to send the datagram on the spot, because the socket might be writable. Have a look at how uv_try_write is used in stream_wrap.cc.


>>>mcollina, Member


I'm ðŸ‘ on that. I'll get it done after this one.


>>>thefourtheye, Contributor



;##D30 It would be better to run this in strict mode
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Better run all these in strict mode


>>>mcollina, Member


I have done a pass of performance optimization in JS-land, which result in better performance for the single buffer use case (and offset/length is now on par with master).


Here are the full results:
https://gist.github.com/mcollina/e8210d26ca6ef2630e23



;##D31 Passing an array of buffers is slower than using Buffer.concat
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Unexpectedly, passing multiple buffers in an array is slower (benchmark/dgram/headers.js) than calling Buffer.concat() in js land for a large payload:




The small buffer case (up to 256 bytes) is way faster than calling Buffer.concat().


Any ideas why this is happening? I expected it to be faster.


>>>thefourtheye, Contributor


@trevnorris


>>>trevnorris, Contributor



;##D32 fixAndSumBuffers() would be a better name
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

nit: function name is slightly incomplete? maybe something like fixAndSumBuffers(). this is nothing serious, and not a blocker.


>>>trevnorris, Contributor



;##D33 This function can be hoisted
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

The only variable I see here that's being used is self. Since 'once' is called on the self object it will also be the this in the callback. So this function can be hoisted.


>>>trevnorris, Contributor



;##D34 Check for instanceof Buffer would be cheaper than wrap the buffer in an array
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN ENU

It's cheaper to check for instanceof Buffer (or HasInstance() from C++) than to always wrap the buffer in an array.


>>>mcollina, Member



;##D35 It seems to considerably slow down runtime
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D34
;##SEN NEU

I have tried this and it turns out 20% slower on my bench. Would you mind clarifying it a bit better?


>>>trevnorris, Contributor


naw. don't worry about it now. i'll take a look later. if there were savings they'd be minimal at best.


>>>trevnorris, Contributor



;##D36 This check is not needed sicne it's redundant
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

No need for this. Both Buffer::Length() and Buffer::Data() already have this check.


>>>trevnorris, Contributor



;##D37 Can we just check if callback is undefined here?
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

Since the ||, why not just check if (callback === undefined) and remove the need for arguments?


>>>trevnorris, Contributor



;##D38 Using a linked list here would reduce overhead
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Optimal case you'd allow the user to pass a string to C++, copy out the memory there and store a pointer to it on the class. Then free the memory in the destructor. This would have to be a linked list, since you're accepting an array of values, but would remove the additional overhead of creating another Buffer object and the GC cost of cleaning it up.


>>>mcollina, Member


Agreed, let's finalize this and then I can submit another PR to handle the strings. However, there is the encoding issue to be solved.


>>>trevnorris, Contributor



;##D39 Offset checks are not needed anymore
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

The offset checks here are no longer necessary since we've moved to typed arrays.


>>>trevnorris, Contributor


Finished quick review.


>>>mcollina, Member


@trevnorris thanks for the feedbacks! I have implemented most of them, and they works :D.


However, it does not solve the slowdown when using the multiple buffer API vs using Buffer.concat().
If you look at:




You'll see that the more elements there are in the array, the slower the array implementation gets vs the Buffer.concat() one.



;##D40 Can libuv be slowing down the implementation?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL REFR D35
;##SEN NEU

@saghul might be something on libuv side?


>>>saghul, Member



;##D41 The only thing happening in libuv is a malloc if there are more than 4 buffers
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D40
;##SEN NEW

On the libuv layer we use sendmsg regardless of the number of buffers. There is however one small difference: if there are more than 4 buffers we need to malloc an array of uv_buf_t structures (just the structures, not the data).


>>>mcollina, Member



;##D42 Increasing the limiti to 16 doesn't improve
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D41
;##SEN NEU

@saghul I've tried increasing that parameter to 16, but things does not change when sending 8 reqs down to libuv.


Even removing the fixBuffer loop in js-land does not increase the speed, so it's either something in my bench, or something that is slowing things down in C++/C, or something more weird that I do not understand.


As a safety check I'll run my benchmark on Linux (virtualized), just to check if it's a Mac OS X issue.


>>>mcollina, Member



;##D43 This seems to be cause by OSX
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D42
;##SEN NEU

Confirmed, it's a Mac OS X issue. On Linux it is 20% faster to send a list of messages than Buffer.concat(). There should be a sendmsg issue on Mac OS X.


@saghul @trevnorris should this be worked around here or in libuv?


>>>saghul, Member


I don't see how, to be honest. There is no extra magic we use.


>>>mcollina, Member


You don't see why this happens, or you don't see how to work around it?

;##D44 We could either fix it in JS-land or use malloc and memcpy on every buffer in libuv
;##ROLE OP
;##INV F
;##BCOM T
;##FORM ENU
;##REL ELAB D43
;##SEN NEU

Anyway, my proposal is to work around it in JS-land. It is a very bad
'fix', but it's quick to implement. A longer term fix is probably to malloc
and memcpy all the buffers in libuv.

;##D45 Is it possible that it affects every sendmsg?
;##ROLE OP
;##INV F
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

Is it possible that all the sendmsg calls with multiple buffers on OS X are
slower, or should it be a udp only thing?


>>>trevnorris, Contributor

;##D46 We could either use WriteWrap or  keep a referenc eto all buffers and create a list
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM ENU
;##REL REFR D44
;##SEN NEU

If you take a look at WriteWrap you'll see storage_size_. What this class does is allocate itself + the amount needed for the remaining data to be sent (look at WriteWrap::New() in
src/stream_base-inl.h). Then the data is copied into the extra allocated space. May be of use, but may also be faster to simply keep a reference to all Buffers and create a list of uv_buf_t.


>>>saghul, Member




A longer term fix is probably to malloc and memcpy all the buffers in libuv.





;##D47 Using malloc + memcpy would go against the core design of libuv
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D46
;##SEN NEW

I don't see this happening. It goes against the core design principles in libuv. We never copy buffers.




Is it possible that all the sendmsg calls with multiple buffers on OS X are slower, or should it be a udp only thing?




It could be a kernel thing in OSX, yeah :-S


>>>mcollina, Member



;##D48 Solved this using Buffer.concat() in JS-land
;##ROLE OP
;##INV F
;##BCOM T
;##FORM SOL
;##REL ELAB D44
;##SEN NEU

@trevnorris @saghul I'm calling Buffer.concat() in js-land, as it is faster that way for 1024 messages (possibly because the memory was already allocated somewhere else). Passing multiple buffers is 20% faster on Linux and on par on Mac OS X (~10% faster than current implementation, due to other perf fixes).


Let me know if you are happy with this, so that I will get the new signatures for send documented.


>>>trevnorris, Contributor


nit: since neither of these can be < 0, mind doing offset >>> 0 instead?


>>>trevnorris, Contributor


missing some semicolons. make sure to run lint. :)


>>>trevnorris, Contributor


style nit: two spaces between functions please.


>>>trevnorris, Contributor


style nit: use {} on both if and else.


>>>trevnorris, Contributor



;##D49 Can you give any detail about what to check on OSX?
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM OPQ
;##REL REFR D48
;##SEN NEU

iirc you gave a specific reason why it's slower on os x. mind commenting on that here so future devs know what to look for when checking for the same optimization?


/cc @bnoordhuis have any comments on platform specific optimizations like this?


>>>indutny, Owner


Just to place the comment at the right line. I disagree with presence of isDarwin branch here.


>>>trevnorris, Contributor


@mcollina looking good. left just a few minor comments.


>>>ronkorving, Contributor


The same style nit as what @trevnorris points out above. The lack of curly braces here makes this block a bit hard to reason about (especially after the throw).


>>>ronkorving, Contributor


Unused variable?


>>>saghul, Member



;##D50 Have wested this on all platforms?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL REFR D48
;##SEN NEU
;##D51 It would be better to let the OS handle it
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D48
;##SEN NEU

I'm not a fan of the if-darwin-this-else-that thing. Have we tested it on all supported platforms? If Apple improves the performance by 500% how would we know? IMHO it's better to let the OS handle it and add a note in the documentation about the fact that in some platforms it might be slower, so users should run their own benchmarks if their application is performance critical.


>>>indutny, Owner


I totally agree with @saghul on the OS X.


>>>ronkorving, Contributor


Me too. Perhaps better to file a bug report with Apple.


>>>mcollina, Member

;##D52 It has not be tested on windows
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D50
;##SEN NEU

;##D53 OSX migth be allocating the memory somewehre
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D49
;##SEN NEU

@saghul I have not tested it on Windows, but I can do it if you want.
I wanted to dig more in what the problem is, and it is definitely an OS X thing - I'd guess they are allocating the memory somewhere (at least it shows the same performance pattern when I do the malloc myself).



;##D54 We can also remove the isDarwin flag for OSX
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D51
;##SEN NEU

On the other side, I'm fine with leaving without the isDarwin flag and be slower on OS X, or putting it in and be faster. Any strong opinion on keeping the isDarwinÂ flag?


>>>mcollina, Member


Nits fixed, thanks @ronkorving @trevnorris and @indutny for reviewing.


Let me know about the isDarwin flag.


>>>ronkorving, Contributor



;##D55 We should remve the isdarwin flag
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D54
;##SEN NEU

I think the past few comments in the main comment thread didn't really support the isDarwin flag. I'm not really in favor. I think Apple needs to fix this, not us. (not saying I'm not sensitive to this issue and what you're trying to achieve, but as @saghul pointed out, we'll never realize it when Apple improves performance).



;##D56 Has this been bencharmed on linux/windows?
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM OPQ
;##REL REFR D52
;##SEN

I would like to ask though, has this been benchmarked on Linux and Windows? Is it really OSX only? Or has that been the only OS tested?


>>>mcollina, Member



;##D57 It has been tested on linux but not windows
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D56
;##SEN NEU
;##D58 Should it be tested on windows?
;##ROLE  OP
;##INV T
;##BCOM T
;##FORM OPQ
;##REL ELAB D57
;##SEN NEW

@ronkorving the reason why I dig into this rabbit hole was because I could not explain why passing an array was slower on my box (OS X). I have also tested on Linux (virtualized), and passing an array is 20% faster than calling concat(). I have not tested on Windows, should I?


There is a nice benchmark so we can test if things improve.


Yes, we should probably report this to Apple.


>>>saghul, Member




@saghul I have not tested it on Windows, but I can do it if you want.





;##D59 There's no reason to test on windows
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D57
;##SEN NEU

I don't see the need, different Windows versions could yield different results, same goes even for Linux.




I wanted to dig more in what the problem is, and it is definitely an OS X thing - I'd guess they are allocating the memory somewhere (at least it shows the same performance pattern when I do the malloc myself).




It could be coincidence.




On the other side, I'm fine with leaving without the isDarwin flag and be slower on OS X, or putting it in and be faster. Any strong opinion on keeping the isDarwin flag?




As I mentioned earlier, I'm -1 on the flag.


>>>indutny, Owner



;##D60 Could the bad performance be caused by an rtifact of the benchmark?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL REFR D53
;##SEN NEU

@mcollina did I understand you right: it is slower on both linux and os x? Could this slowness be an artifact of benchmarking?


>>>mcollina, Member


No. It is slower only on Mac OS X, and I have not tested on Windows. On
Linux passing multiple bufs is faster than calling Buffer.concat. I did not
express myself clearly.
Il giorno mer 6 gen 2016 alle 20:14 Fedor Indutny notifications@github.com
ha scritto:




@mcollina https://github.com/mcollina did I understand you right: it is
slower on both linux and os x? Could this slowness be an artifact of
benchmarking?


â€”
Reply to this email directly or view it on GitHub
#4374 (comment).




>>>mcollina, Member


I have removed the isDarwin flag and added the docs for this new feature.


>>>ronkorving, Contributor


Trailing period is missing (wasn't there before either, but makes sense though, no?)


>>>reqshark, Other


indeed that would be appropriate


also i would replace the current use of or with a comma like





>>>mcollina, Member


Thanks, updated.


>>>indutny, Owner


Why this?


>>>mcollina, Member


Because @trevnorris asked me to #4374 (comment).
I'm not sure why the | 0 was there in the first place.


>>>indutny, Owner



;##61 >>> 0 should be replaced with ===0
;##ROLE PM
;##INV T
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

The thing is that the code below checks port <= 0, which should be transformed to === 0, because >>> 0 always yields non-negative integer.


>>>mcollina, Member


should I update the check below so that port === 0?


>>>indutny, Owner


Yeah, I think so. Thank you!


>>>ronkorving, Contributor



;##D62 If a port is negative, >>> will not yield 0
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D61
;##SEN NEU

If port is negative, >>> will not yield 0.




Please be careful with this.


>>>reqshark, Other



;##D63 Passing a negative port is rare
;##ROLE ETC
;##INV F
;##BCOM T
;##FORM SOL
;##REL REFR D62
;##SEN NEU

passing a negative port seems rather unusual


>>>mcollina, Member


@indutny check updated.


>>>ronkorving, Contributor



;##D64 That was checked by the previous code
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D63
;##SEN NEU

@reqshark but that's what the previous code checked for: <= 0


>>>reqshark, Other



;##D65 UDP is still vulnerable toward it being connectionless
;##ROLE ETC
;##INV T
;##BCOM T
;##FORM SOL
;##REL REFR D64
;##SEN NEG

@ronkorving, that's right. good thing UDP is a connectionless, "unreliable protocol". so I'm sure any danger of sendto() aiming toward a negative port is mitigated by that fact.


>>>mcollina, Member


Squashed and updated with all the nits. I think this is ready, would you mind passing through it one last time?


>>>jasnell, Owner


const?


>>>jasnell, Owner


const here too :-)


>>>jasnell, Owner



;##D66 All thr require in the benchmark tests should use const
;##ROLE PM
;##INV F
;##BCOM T
;##FORM SOL
;##REL NEW
;##SEN NEU

Overall LGTM. A few minor nits tho... can you please use const for all the require() calls in the benchmark tests :-)


>>>mcollina, Member


@jasnell updated, thanks :D.


>>>jasnell, Owner


CI: https://ci.nodejs.org/job/node-test-pull-request/1219/


>>>jasnell, Owner


Failures in CI look unrelated. @trevnorris @thefourtheye @saghul ... any further comments on this?


>>>saghul, Member


I noticed one thing while reviewing the C++ side one last time. This PR adds a new parameter to the send callback, with the size of the sent data, see: https://github.com/nodejs/node/pull/4374/files#diff-b62464f44488c6247346b82a87cbd20aR266 and https://github.com/nodejs/node/pull/4374/files#diff-b62464f44488c6247346b82a87cbd20aR360



;##D67 Do we want to add this parameter to the send callback? It is undocumented
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

Is this an API change we want to make? If so, it's undocumented AFAIS. Since this is UDP, there is no way for that value to be different from the length of the sent buffers, because either the whole datagram goes or doesn't, there is no in between.


>>>mcollina, Member

;##D68 That parameter has been there for a long time
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D67
;##SEN NEU

;##D69 The change is that it is dealt with in C++ for speed
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D68
;##SEN NEU

@saghul no it is not a new parameter, it has been there for a very long time: see https://github.com/nodejs/node/pull/4374/files#diff-5c0ca4f44209cc4cc68c4dccadbd4a07L346.
There are also (old) unit tests for this.
What did change was that this parameter was kept in JS-land, while now is dealt with in C++ for speed.


>>>mcollina, Member


also cc @mafintosh who has done some UDP work, and might provide some useful feedback.


>>>saghul, Member


@mcollina doh, sorry about that! In my defense, that wan't obvious at all :-)


>>>saghul, Member


LGTM.


>>>mcollina, Member


@saghul no worries, it wasn't obvious at all. Good catch anyway, so it remains track of that in here.


>>>jasnell, Owner


awesome... I'd still like to have @trevnorris sign off before this lands given his review


>>>mcollina, Member


@jasnell I agree, I have been waiting for some days.


What is the process for landing semver-minor into the v5 line? I couldn't really find it.


>>>cjihrig, Owner




What is the process for landing semver-minor into the v5 line? I couldn't really find it.




It will land in master. The next time a v5 release is cut, it will be cherry picked over.


>>>mafintosh, Member



;##D70 Would I need check the node version to feature detect this?
;##ROLE PM
;##INV T
;##BCOM T
;##FORM OPQ
;##REL NEW
;##SEN NEU

@mcollina great to see you working improving dgram a bit :) this would definitely be useful for my utp library as all writes currently require a copy in node to prepend a protocol header. to feature detect for this would i need to check which version of node is running?


>>>mcollina, Member



;##D71 We might add a quick ponyfill for not requiring it
;##ROLE OP
;##INV T
;##BCOM T
;##FORM SOL
;##REL ELAB D70
;##SEN NEU

Unfortunately yes. I think we might write a quick ponyfill to do this wherever it is needed.


>>>mcollina, Member


@trevnorris would you mind having a look at this again, so we can land it?


>>>mcollina, Member


Any other LGTMs, so we can land this?


>>>jasnell, Owner


I'd say go for it. CI looked fine after the last run.


>>>mcollina, Member


Giving CI a second run: https://ci.nodejs.org/job/node-test-pull-request/1432/.


>>>mcollina, Member


Landed in 137f53c


>>>jasnell, Owner


Thank you!
On Jan 29, 2016 10:32 AM, "Matteo Collina" notifications@github.com wrote:




Landed in 137f53c
137f53c


â€”
Reply to this email directly or view it on GitHub
#4374 (comment).




>>>ronkorving, Contributor


Great job! Thanks


>>>mcollina, Member


So happy this is released!! Thx to everyone!


